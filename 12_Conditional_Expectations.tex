\documentclass[12pt]{article}

\input{prob_preamble.tex}
\externaldocument{3_ProbabilitySpaces}
\externaldocument{4_Random_Variables}
\externaldocument{5_Convergence_and_Independence}
\externaldocument{6_Borel_Cantelli_Lemmas}


\begin{document}

\handout{12}{Conditional Expectations and Martingales}

\section{Definition}
Given a probability space $(\Omega,\scA,\P)$, let us start off with a simple example that we are familiar with from undergraduate probability courses. Suppose we have a r.v.\ $X$ and a discrete r.v.\ $Y \in \{1,2,\ldots\}$. We can partition $\Omega = \bigcup_{y \geq 1 } \Omega_y$, where $\Omega_y = \{ \omega: Y(\omega)=y\}$.  Then the conditional expectation of $X$ given $Y$ can be defined as
\begin{align*}
\E[X]{Y=y} = \frac{\E[X\indicatore{\Omega_y}]}{\P (\Omega_y)}
\end{align*}
for each value of $y$. Note that the expectation is conditioned on a set $\Omega_y=\{Y=y\}$. It depends on the value $Y(\omega)=y$, and is hence a function of $\omega$. Since $\Omega_y$ is measurable, this is a measurable function and is hence a \emph{random variable}! 

We wish to generalize this definition to all sets in a sub-$\sigma$-algebra in $\scA$. Furthermore, when we average out the conditional expectation over a set $B$ of feasible $Y$ values, we should get back the expectation over this set:
\begin{align*}
\sum_{y\in B} \E[X]{Y=y} \P(Y=y) = \sum_{y\in B} \E[X\indicatore{\Omega_y}] = \E[X\indicatore{Y\in B}],
\end{align*}
where the last inequality follows from Fubini's theorem.
	
\begin{Definition}\label{wk12:expectation}
Suppose $\E |X|<\infty$ and the $\sigma$-algebra $\scF \subset \scA$. A random variable $Y:\Omega \mapsto \Real$ is a conditional expectation of $X$ given $\scF$ if
\begin{enumerate}[(i)]
\item $Y^{-1}(B)\in \scF, \ \forall B \in \scB(\Real)$, i.e., $Y\ is\ \scF$-measurable (we denote it as $Y\in\scF$).
\item $\forall A \in \scF$, $\E[Y\indicatore{A}] = \int_{A} Y \ud \P=\int _A X \ud \P = \E[X\indicatore{A}]$.
\end{enumerate}
\end{Definition}
If $Y$ is a conditional expectation of $X$ given $\scF$, we write $Y = \E[X]{\scF}$. We also write $\E[X]{Y}=\E[X]{\sigma(Y)}$, where $\sigma(Y)$ is the $\sigma$-algebra generated by $Y$. The notion that expectation is an operator comes from here: $\E[{\cdot} \given \scF] : L^1(\Omega,\scA,\P) \mapsto L^1(\Omega,\scF,\P)$ is a linear (which will be shown later) transformation.

The \textbf{existence} of conditional expectations is given by Radonâ€“Nikodym Theorem (\cref{wk4:Radon-Nikodym Theorem}). Suppose $X\geq0$. We can define a measure
\begin{align*}
\mu(A)= \int_A X\ud \P, \text{ where $A\in\scF$ and } \mu \ll \P.
\end{align*}
Since $X$ is integrable, $\mu$ is a finite measure. Then there exists $Y= \dfrac{\ud \mu}{\ud \P} \in \scF$ such that $\int_A X\ud \P=\mu(A)=\int_A Y \ud \P$. The existence of conditional expectations for general $X = X^+-X^-$ now follows. 

We next show that conditional expectations are \textbf{unique} almost surely. Suppose that $Y$ and $Y'$ are both versions of $\E [X \given \scF]$ and $\P (Y\neq Y')>0$, i.e., $\P (Y> Y')>0$ or $\P (Y< Y')>0$. Let $A=\{Y>Y'\}\in \scF$ and suppose that $\P(A)>0$. Then, we have
\begin{align*}
0<\E[(Y-Y')\indicatore{A}] &= \E Y\indicatore{A}-\E Y'\indicatore{A}\\
&=\E X\indicatore{A} -\E X\indicatore{A}\\
&= 0, 
\end{align*}
which is a contradiction. A similar argument holds for the case $\P (Y< Y')>0$. Therefore, $\P (Y\neq Y')=0$, and $Y=Y' \as$


\begin{Example}\label{wk12:ex:jointpdf}
Suppose that the joint pdf of $(X,Y)$ is $f(x,y)$. Let 
\begin{align*}
h(y)=\int g(x)f(x|y)\ud x =\frac{\int g(x)f(x,y)\ud x}{\int f(x,y) \ud x}.
\end{align*}
We show that $h(Y)=\E[g(X)]{Y}$. Let $A \in \sigma(Y)$. Then $A=\{\omega:Y(\omega)\in B\}$ for some $B \in \scB(\Real)$. We check that
\begin{align*}
\E[h(Y)\indicatore{A}]
&=\int_B \int h(y) f(x,y)\ud x \ud y\\
&=\int_B h(y) \int f(x,y)\ud x \ud y\\
&=\int_B \int g(x)f(x,y)\ud x \ud y\\
&=\E [g(X)\indicatore{B}(Y)]\\
&=\E [g(X)\indicatore{A}],
\end{align*}
and the claim is proved.
\end{Example}

\begin{Example}
A sensor makes an observation $X \in \Real$ and sends a summary $Z = \gamma(X) \in \Real$ to a fusion center, where $\gamma$ is a randomized function. The fusion center uses $Z$ to perform hypothesis testing for
\begin{align*}
H=\left\{
\begin{array}{ll}
H_0:\ &X\sim \P_0, \\
H_1:\ &X\sim \P_1.
\end{array}
\right.
\end{align*} 
We assume that $\P_0$ and $\P_1$ are absolutely continuous w.r.t.\ each other. Note that these are the laws of $X$ under different hypotheses.

For $i=0,1$, let $\P_{i,Z}$ be the restriction of $\P_i$ on $\sigma(Z)$. Suppose $A\in \sigma(Z)$. We have 
\begin{align*}
\int_A \frac{\ud \P_{1,Z}}{\ud \P_{0,Z}}\ \ud \P_0&=\int_A \frac{\ud \P_{1,Z}}{\ud \P_{0,Z}}\ \ud \P_{0,Z} \\
&= \int _A \ud \P_{1,Z}\\
&= \int_A \ud \P_1 \\
&= \int_A \frac{\ud \P_1}{\ud \P_0 }\ud \P_0.
\end{align*}
Therefore,
\begin{align*}
\frac{\ud \P_{1,Z}}{\ud \P_{0,Z}}=\E[\frac{\ud \P_1 }{\ud \P_0 } \given \sigma(Z)].
\end{align*}
As a special case, suppose $X$ and $Z$ are continuous r.v.s. Then $\gamma$ corresponds to a conditional pdf $p(z|x)$ and letting $f_i$ be the pdf of $\P_i$, $i=0,1$, we have from \cref{wk12:ex:jointpdf},
\begin{align*}
\E_{0}[\frac{f_1(x)}{f_0(x)} \given Z=z] 
&=\frac{\int \frac{f_1(x)}{f_0(x)} f_0(x,z)\ud z}{\int f_0(x,z)\ud x}\\
&=\frac{\int {f_1(x)} p(z|x)\ud x}{\int f_{0,Z}(z)}\\
&=\frac{ f_{1,Z}(z)}{ f_{0,Z}(z)}.
\end{align*}
\end{Example}


\section{Properties}
In the following, we list some fundamental properties of conditional expectations, many without proofs. The proofs are left for your exercise.

\begin{enumerate}[1.]
\item If $\sigma(X)\subset \scF$, then $X \in \scF$ and by \cref{wk12:expectation}, $X=\E[X]{\scF}$ a.s. As a special case, we have $\E[X]{\scA}=X$. If $c$ is a constant, viewed as a r.v., its $\sigma$-algebra is the trivial one and so $\E[c]{\scF}=c$.

\item If $\sigma(X)\independent \scF$, then for $A\in \scF$, $\E[X\indicatore{A}]=\E X\E\indicatore{A}=\int_A \E X\ud \P \Rightarrow \E [X \given \scF] = \E X$ a.s. As a special case, if $\scF=\{\emptyset,\Omega \}$, we have $\E[X]{\scF}=\E X$.

\item Suppose that $c$ is a constant, then $\E [cX+Y \given \scF]=c\E[X \given \scF] + \E[Y \given \scF]$. 
\item Suppose $\scG$ and $\scF$ are $\sigma$-algebras with $\scG\subset\scF$, then $\E[X \given \scG]=\E[{\E[X \given \scG]} \given \scF]=\E[{\E[X \given \scF]} \given \scG]$.
\begin{proof}
Let $A\in\scG\subset\scF$, then we have
\begin{align*}
\E[{\E[{\E[X \given \scF]} \given \scG]}\indicatore{A}]
&= \E[{\E[X \given \scF]}\indicatore{A}] \text{ by definition of $\E[\cdot]{\scG}$}\\
&= \E X\indicatore{A} \text{ by definition of $\E[{\cdot} \given \scF]$}.
\end{align*}
\end{proof}
For example, setting $\scG=\{\emptyset,\Omega\}$, we obtain $\E[\E[X]{\scF}] = \E X$ for all $\scF$. 

\item If $X\leq Y$ a.s., then $\E[X \given \scF]\leq \E [Y \given \scF]$.
\begin{Lemma}\label{wk12:conditional_rel}
$\E[X \given \scF]\leq \E [Y \given \scF]$ a.s.\ iff $\E X\indicatore{A} \leq \E Y\indicatore{A}$ for all $A\in\scF$.
\end{Lemma}
\begin{proof}
Similar to the uniqueness proof.
\end{proof}

\item Monotone Convergence Theorem. Suppose $\E |X_n|<\infty,\ \forall n\geq 1,\ \E|X|<\infty$ and $X_n \uparrow X$ a.s. as $n \to \infty$, then $\E[X_n \given \scF] \uparrow \E[X \given \scF]$ a.s.
\begin{proof}
Since $X_n \uparrow X$, we have
\begin{align*}
&\E[X_n \given \scF]\leq \E[X_{n+1} \given \scF]\leq \E[X \given \scF]\\
\intertext{and there exists}
&Y\triangleq \lim_{n\to \infty} \E [X_n \given \scF]\leq \E [X \given \scF].
\end{align*}
From \cref{lem:wk4:infX_rv}, $Y$ is $\scF$-measurable. Moreover, for each $A\in\scF$, we have
\begin{align*}
\E[X_n \given \scF]\indicatore{A} \uparrow  Y\indicatore{A}
\end{align*}
since $\E[X_n \given \scF] \uparrow Y$. From the MCT, we obtain
\begin{align*}
\E[\E[X_n]{\scF}\indicatore{A}] \to \E[Y\indicatore{A}].
\end{align*}
But $\E[\E[X_n]{\scF}\indicatore{A}]=\E[X_n\indicatore{A}] \xrightarrow{\text{MCT}} \E[X\indicatore{A}]$. Therefore, $\E[Y\indicatore{A}]=\E[X\indicatore{A}]$ and $Y=\E[X \given \scF]$ a.s. 
\end{proof}

\item Dominated Convergence Theorem. If $|X_n|\leq Y \as,\ \E Y<\infty$ and $X_n \to X$ a.s., then $\lim_{n\to \infty}\E[X_n \given \scF]=\E[X \given \scF]$.

\item Suppose $X\independent Y$ and $\E|\phi(X,Y)|<\infty$. Let $g(y)=\E [\phi(X,y)]$, then $\E[\phi(X,Y) \given Y]=g(Y)$.

\item If $\E|X|<\infty$, $\E|XY|<\infty$, and $Y\in \scF$, then $\E[XY \given \scF]=Y\E[X \given \scF]$.
 
\item The usual inequalities apply. We define $\P(A \given \scF) = \E[\indicatore{A} \given \scF]$.
\begin{itemize}
	\item Markov's inequality: For $a>0$, $\P(X>a \given \scF) \leq \odfrac{a}\E[X \given \scF]$.
	\item Chebyshev's inequality: For $a>0$, $\P({|X|\geq a} \given \scF) \leq \odfrac{a^2}\E[X^2 \given \scF]$.
	\item Cauchy-Schwarz inequality: $\E[XY \given \scF]^2 \leq \E[X^2 \given \scF] \E[Y^2 \given \scF]$.
	\item Jensen's inequality: Given a convex function $\phi$ with $\E|\phi(X)|<\infty$, we have 
	\begin{align*}
	\phi(\E[X \given \scF]) \leq \E[\phi(X) \given \scF].
	\end{align*}
\end{itemize}

\end{enumerate}

\section{\texorpdfstring{$L^2$}{L2} Interpretation}

\begin{Proposition}
Consider $L^2(\Omega,\scF,\P)=\{X : X \in \scF,\ \E X^2 <\infty\}$, which is a subspace of $L^2(\Omega,\scA,\P)$. If $X\in L^2(\Omega,\scA,\P)$, then 
\begin{align*}
\E[X \given \scF] = \argmin_{Y\in L^2(\Omega,\scF,\P)} \E(X-Y)^2.
\end{align*}
\end{Proposition}
\begin{proof}
For $Y\in L^2(\Omega,\scF,\P)$, let $Z=\E[X \given \scF]-Y \in \scF$. Then, we have
\begin{align*}
\E(X-Y)^2
&=\E[(X-{\E[X \given \scF]}+{\E[X \given \scF]}-Y)^2]\\
&=\E(X-{\E[X \given \scF]})^2+ \E Z^2 + 2\E[Z(X-{\E[X \given \scF]})]
\end{align*}
and since $Z\in\scF$,
\begin{align*}
\E[Z(X-{\E[X \given \scF]})]
&=\E[ZX] - \E[\E[ZX]{\scF}]\\
&=\E[ZX] - \E[ZX]\\
&=0.
\end{align*}
Therefore,
\begin{align*}
\E(X-Y)^2 &\geq \E(X-\E[X]{\scF})^2,
\end{align*}
and the proposition is proved.
\end{proof}
We can define $\var(X \given \scF) = \E[(X-{\E[X \given \scF]})^2 \given \scF] = \E[X^2 \given \scF] - \E[X \given \scF]^2$. One can show (exercise) that
\begin{align*}
\var(X) = \E[\var(X \given \scF)] + \var(\E[X \given \scF]).
\end{align*}

\section{Martingales}
Consider a probability space $(\Omega,\scF,\P)$. A sequence of sub-$\sigma$-algebras $\scF_0\subset \scF_1\subset \scF_2\subset \ldots \subset \scF$ is called a \emph{filtration}. Let the r.v.\ $M_n\in \scF_n$ (i.e., $M_n$ is $\scF_n$-measurable). We say that $M_n$ is adapted to $\scF_n$. 
	
\begin{Definition}[Martingale]\label{wk12:Martingale} 
We say that $(M_n,\scF_n)_{n\geq 0}$ is a martingale if $\E|M_n|<\infty$ for all $n\geq0$ and
\begin{align}\label{mcondition}
\E[M_m \given \scF_n]=M_n,\ \forall m\geq n.
\end{align}
\end{Definition}
By induction, the condition \cref{mcondition} is equivalent to $\E[M_n \given \scF_{n-1}] = M_{n-1}$. We give examples of martingales below.

\begin{Example}\label{ex:wk12_mex_1}
Suppose that $(X_n)_{n\geq1}$ are independent r.v.s, $\scF_n=\sigma(X_1,\ldots,X_n)$ and $\E X_n=0$. Let $S_n=\sum_{i=1}^n X_i$. Then,
\begin{align*}
\E [S_n \given \scF_{n-1}]&= \E [S_{n-1}+X_n \given \scF_{n-1}]\\
&=S_{n-1}+\E [X_n \given \scF_{n-1}]
\end{align*}
Since $\E [X_n \given \scF_{n-1}] = \E X_n =0$, we have
\begin{align*}
\E [S_n \given \scF_{n-1}]&=S_{n-1}.
\end{align*}
Therefore, $(S_{n},\scF_n)_{n\geq1}$ is a martingale.
\end{Example}

\begin{Example}\label{ex:wk12_mex_2}
Suppose that $(X_n)_{n\geq1}$ are independent r.v.s, $\var(X_n)=\sigma^2$ and $\E X_n=0$. Let $M_0=0$, and $M_n=S_n^2-n\sigma^2$. We have
\begin{align*}
\E[M_n \given \scF_{n-1}]
&= \E[S_{n-1}^2 + 2S_{n-1}X_n + X_n^2 - n\sigma^2  \given  \scF_{n-1}]\\
&= S_{n-1}^2 - (n-1)\sigma^2\\
&= M_{n-1},
\end{align*}
and $(M_{n},\scF_n)_{n\geq1}$ is a martingale.
\end{Example}

\begin{Example}\label{ex:wk12_mex_3}
Suppose that $(X_i)_{i\geq1}$ are independent r.v.s., $X_i\geq 0$ and $\E X_i=1$. Let $M_0=1$, and $M_n=\prod_{i=1}^n X_i$. We have
\begin{align*}
\E [M_n \given \scF_{n-1}]&= \E [M_{n-1}\cdot X_n \given \scF_{n-1}]\\
&=M_{n-1} \E [X_n \given \scF_{n-1}]
\end{align*}
Since $\E [X_n \given \scF_{n-1}] = \E X_i =1$, we have
\begin{align*}
\E [S_n \given \scF_{n-1}]&=M_{n-1},
\end{align*}
and $(M_{n},\scF_n)_{n\geq1}$ is a martingale. 
\end{Example}


\begin{Example}\label{ex:wk12_mex_4}
Suppose that $(Y_n)_{n\geq1}$ are i.i.d. and $\phi(\lambda)=\E e^{\lambda Y_1}<\infty$. Let $X_n=\dfrac{e^{\lambda Y_n}}{\phi(\lambda)}$. Then $\E X_n=1$. Let 
\begin{align*}
M_n=\frac{\exp{(\lambda \sum_{i=1}^n Y_n)}}{\phi(\lambda)^n}
\end{align*} 
and from \cref{ex:wk12_mex_3}, we have $(M_{n},\scF_n)_{n\geq1}$ is a martingale.
\end{Example}

\begin{Example}\label{ex:wk12_mex_5}
Given $\E|X|<\infty$ and a filtration $(\scF_n)_{n\geq1}$, let $M_n=\E[X \given \scF_n]$. Then, 
\begin{align*}
\E [M_n \given \scF_{n-1}]
&=\E [{\E[X \given \scF_n]} \given \scF_{n-1}]\\
&=\E[X \given \scF_{n-1}]\\
&=M_{n-1}.
\end{align*}
Therefore, $(M_{n},\scF_n)_{n\geq1}$ is a martingale.
\end{Example}

\begin{Definition}
$(A_n)_{n\geq1}$ is predictable w.r.t.\ $(\scF_n)_{n\geq 0}$ if $A_n \in \scF_{n-1}, \forall n  \geq 1$.
\end{Definition}

We call $(\widetilde{M}_n)_{n\geq0}$ the martingale transform of $(M_n)_{n\geq0}$ by $(A_n)_{n\geq1}$ if
\begin{enumerate}[(i)]
\item $\widetilde{M}_0 = M_0$. (In general, we can choose any integrable $\widetilde{M}_0\in\scF_0$.)
\item $\widetilde{M}_n = \widetilde{M}_0 + \sum_{k=1}^n A_k(M_k-M_{k-1})$. 
\end{enumerate}
%
\begin{Theorem}[MTT] \label{wk12:MTT}
If $(A_n)_{n\geq1}$ is predictable w.r.t.\ $(\scF_n)_{n\geq0}$ and bounded, and $(M_n,\scF_n)_{n\geq0}$ is a martingale, then $(\widetilde{M}_n, \scF_n)_{n\geq0}$ is a martingale.
\end{Theorem}
\begin{proof}
It is obvious that $\widetilde{M}_n \in \scF_n$ and $\E\abs{\widetilde{M}_n} < \infty$. Furthermore, we have 
\begin{align*}
\E[\widetilde{M}_n - \widetilde{M}_{n-1} \given \scF_{n-1}]
&= \E[A_n(M_n - M_{n-1}) \given \scF_{n-1}] \\
&= A_n\E[M_n - M_{n-1} \given \scF_{n-1}] = 0.
\end{align*}
\end{proof}

\begin{Example}
Suppose we divide time into discrete equal intervals (e.g., days). Let $M_n$ be the price of a stock at time instant $n\geq1$ and $\scF_n$ be the available information up to time $n$. From the efficient-market hypothesis (assuming no dividends and discount factor), $M_n$ is a ``fair'' price that has priced in all expected future gains or losses, i.e., $\E[M_m \given \scF_n] = M_n$ for $m\geq n$ and $(M_n,\scF_n)$ is a martingale. Let $(A_n)_{n\geq1}$ be a strategy that decides to hold $A_n$ units of the stock in the time period $[n-1,n)$. Clearly, $(A_n)$ has to be predictable w.r.t.\ $(\scF_n)$. Then $\widetilde{M}_n= \sum_{k\leq n} A_k(M_k-M_{k-1})$ is the change in wealth of this strategy up to time $n$. The MTT says that the expected wealth of \emph{any} strategy is the same and what you start with, i.e., you cannot beat the market! We will see a much stronger version of this result in Doob's Optional Stopping Theorem in the next session.
\end{Example}

%\bibliography{mybib}
\bibliographystyle{alpha}

\end{document}


