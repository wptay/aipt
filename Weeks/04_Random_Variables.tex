\documentclass[../aipt.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}


\externaldocument{01_BasicRealAnalysis_I}
\externaldocument{03_ProbabilitySpaces}


\begin{document}

\handout{4}{Random Variables}

Throughout this note, we consider a probability space $(\Omega,\calA,\P)$.

\section{Measurable Functions}

\begin{Definition}
A function $X :(\Omega,\calA) \mapsto (\Real,\calBR)$ is measurable if $\forall B\in\calBR$, $X^{-1}(B)\triangleq\{\omega \in \Omega:X(\omega) \in B\} \in \calA$. In probability theory, $X$ is called a random variable or random element.
\end{Definition}

\begin{Lemma}\label{wk4:lemma:Xleqt}
$X$ is a random variable iff $\forall t \in \Real$, $\{\omega \in \Omega: X(\omega) \leq t\} \in \calA$.
\end{Lemma}
\begin{proof}
`$\Rightarrow$': Suppose $X$ is a random variable. Since $(-\infty,t]\in \calB$, we have $\{\omega \in \Omega: X(\omega) \leq t\} \in \calA$.

`$\Leftarrow$': Let $\calD=\{B\in\calB: X^{-1}(B)\in \calA\}$. Since
\begin{align*}
X^{-1}(B^c)&=(X^{-1}(B))^c,\\
X^{-1}\parens*{\bigcup_{i\geq1}{B_i}}&=\bigcup_{i\geq 1}{X^{-1}(B_i)},
\end{align*}
$\calD$ is a $\sigma$-algebra. Since $(-\infty,t]\in\calD$ and $\{(-\infty,t]\}$ generates $\calBR$, we have $\calBR\subset\calD$ and $X$ is a random variable.
\end{proof}


\begin{Lemma}\label{lem:wk4:infX_rv}
Suppose $X$ is a random variable, then $\inf_{n\geq 1}{X_n}$, $\sup_{n\geq 1}{X_n}$, $\limsup_{n\to \infty}{X_n}$, $\liminf_{n\to \infty}{X_n}$ are random variables.
\end{Lemma}
\begin{proof}
We show that $\inf_{n\geq 1}{X_n}$ is a random variable. We have for any $t\in\Real$,
\begin{align*}
\left\{\inf_{n\geq 1}{X_n}\leq t\right\}=\bigcup_{n \geq 1}\{X_n \leq t\} \in \calA,
\end{align*}
since each $X_n$ is a random variable. By \cref{wk4:lemma:Xleqt}, the result follows. The proofs for the other claims are similar.
\end{proof}


The law or distribution of a random variable $X$ is given by
\begin{align*}
\P_{X}(B)\triangleq \P(X\in B)
&= \P(\{\omega : X(\omega) \in B\}) \\
&= \P(X^{-1}(B)) \\
&= \P\circ X^{-1}(B),
\end{align*}
and we write
\begin{align*}
\sigma(X) 
&= \text{$\sigma$-algebra generated by $X$} \\
&= \{X^{-1}(B) : B \in \calB(\Real)\}.
\end{align*}
Note that the RHS of the last equality is a $\sigma$-algebra.

Suppose $g:(\Real, \calBR) \mapsto (\Real, \calBR)$ is measurable, i.e., $g^{-1}(B)\in\calBR$ for all $B\in\calBR$, then $g(X)$ is a random variable since 
\begin{align*}
(g \circ X)^{-1}(B)=X^{-1}(g^{-1}(B))\in \calA.
\end{align*}


\section{Expectation}\label{sec:Expectation}

For $A\in\calA$, the indicator function for $A$ is given by 
\begin{align}
\indicatore{A}(\omega)=\left\{
\begin{aligned}
1,\ &	 \text{if } \omega \in A, \\
0,\  & 	\text{otherwise}.
\end{aligned}
\right.
\end{align}

\begin{Definition}
$X(\omega)$ is a simple random variable if there is a discrete set of numbers $x_1<x_2< \ldots<x_n$, such that $X(\omega)= \sum_{i=1}^n{x_i} \indicatore{A_i}(\omega)$, where $A_i\in\calA$, $A_{i}\cap A_{j}= \emptyset$ for $i \neq j$, and $\bigcup_{i=1}^n{A_{i}} = \Omega$.
\end{Definition}


\begin{Definition}\label{wk4:def:expectation}
The expectation (Lebesgue integral) of a simple random variable (measurable function) $X: \Omega \to \Real$ is given by
\begin{align*}
\int_{\Omega} X(\omega) \ud\P
& = \int_{\Omega} X(\omega) \P(\ud\omega) 
= \E[X] 
\triangleq \sum_{i=1}^n{x_i \P(A_i)}.
\end{align*}
\end{Definition}

\begin{Lemma}\label{wk4:lem:B-exp}
For any measurable partition $B_1,B_2,\ldots,B_n$ of $\Omega$ (i.e., for $i \neq j,\ B_{i}\cap B_{j}= \emptyset$ and $\bigcup_{i=1}^n{B_{i}} = \Omega$), with $X(\omega)=\sum_{j=1}^m b_j \indicatore{B_j}(\omega)$, then $\E X= \sum_{j=1}^m{b_j \P({B_j})}$.
\end{Lemma}
\begin{proof}
For $\omega \in A_i\cap B_j$, we have $x_i=X(\omega)= b_j$. Therefore, we obtain
\begin{align*}
\sum_{i=1}^n{x_i \P(A_i)}
& = \sum_{i=1}^n{x_i}\sum_{j=1}^m{\P(A_i\cap B_j)}\\
& = \sum_{i} \sum_{j} {x_i\P(A_i\cap B_j)}\\
& = \sum_{i} \sum_{j} {b_j\P(A_i\cap B_j)}\\
& =\sum_{j} {b_j\P(B_j)}, 
\end{align*}
where the interchange of summations is allowed because the number of terms in the summand is finite.
\end{proof}


The following result follows from \cref{wk4:def:expectation} using similar computation as in \cref{wk4:lem:B-exp}.
\begin{Lemma}\label{wk4:lem:simpleprop}
Suppose $X$ and $Y$ are simple random variables. Then we have:
\begin{enumerate}[(i)]
\item $X(\omega)\leq Y(\omega) \implies \E X \leq \E Y$.
\item For any $a\in\Real$, $aX$ is a simple random variable and $\E[a X]= a \E X$.
\item $X+Y$ is a simple random variable and $\E[X + Y]=\E X + \E Y$.
\end{enumerate}
\end{Lemma}

\begin{Definition}\label{wk4:def:expectation_nz}
If $X \geq 0$, we define
\begin{align*}
\E X = \sup\set*{\E Z \given Z\leq X,\, Z \text{ is simple}}.
\end{align*}
\end{Definition}

Let $X^+=\max(X,0)\geq 0$ and $X^-=-\min(X,0)\geq 0$. We have $X=X^+ - X^-$. We can then define $\E X= \E X^+- \E X^-$, if not both $\E X^+$ and $\E X^-$ are infinite. If $\E X^+$ and $\E X^-$ are both infinite, then the expectation of $X$ is \emph{undefined}.

If $\E|X| = \E X^+ + \E X^-<\infty$, we say that it is integrable. This implies that both $\E X^+$ and $\E X^-$ are finite, hence $\E X$ exists and is finite.

\begin{Lemma}\label{wk4:lem:XlessY}
If $0\leq X \leq Y$, then $\E X \leq \E Y$.
\end{Lemma}
\begin{proof}
For any simple r.v.\ $Z$, since $\{Z\leq X\} \subset \{Z\leq Y\}$, $\E X \leq \E Y$ follows from \cref{wk4:def:expectation_nz}.
\end{proof}

Our goal is to prove the following theorem, which shows that expectations are additive.

\begin{Theorem}\label{wk4:additivity_exp}
Suppose $X,Y\geq 0$, then $\E[X+Y] = \E X+\E Y$. 
\end{Theorem}

We first show the following.
\begin{Lemma} 
Suppose $X,Y\geq 0$, then $\E[X+Y] \geq \E X+\E Y$.
\end{Lemma}
\begin{proof}
$\forall \epsilon>0$, $\exists$ simple random variables $Z_1\leq X$ and $Z_2 \leq Y$ such that
\begin{align*}
\E X & \leq \E Z_1+\frac{\epsilon}{2},\\
\E Y & \leq \E Z_2+\frac{\epsilon}{2},
\end{align*}
so that
\begin{align*}
\E X+\E Y 
&\leq \E Z_1 + \E Z_2 + \epsilon\\
& = \E[Z_1+Z_2]+\epsilon \\
& \leq \E[X+Y]+\epsilon,
\end{align*}
where the first equality follows from \cref{wk4:lem:simpleprop} and the last inequality from $Z_1+Z_2\leq X+Y$ and \cref{wk4:def:expectation_nz}. Since $\epsilon$ can be arbitrarily small, the proof is complete.
\end{proof}

To complete the proof of \cref{wk4:additivity_exp}, we need to show subadditivity: $\E[X+Y] \leq \E X + \E Y$. It turns out that this is highly non-trivial and in the process we learn several nice proof techniques and the very useful theorem below.

\begin{Theorem}[Monotone Convergence Theorem (MCT)]\label{Monotone Convergence Theorem}
Suppose $0 \leq X_n \leq X_{n+1}$, for $n \geq 1$ and $X_n(\omega) \to X(\omega)$ for all $\omega\in\Omega$. Then 
\begin{align*}
\lim_{n\to \infty}\E X_n &= \E[\lim_{n\to \infty}X_n]= \E X.
\end{align*}
\end{Theorem}

We first give an example why we cannot always interchange the order of integration and limit.
\begin{Example}\label{wk4:example1}
Consider the probability space $((0,1), \calB((0,1)),\lambda)$, where $\lambda$ is the Lebesgue measure, and let
\begin{equation}
X_n(\omega)=\left\{
\begin{aligned}
n  &,\text{ if } 		0< \omega \leq \frac{1}{n}, \\
0  &,\text{ if } 		\ofrac{n}< \omega <1. \\
\end{aligned}
\right.
\end{equation}
Then we have $\E X_n = 1$ while $\lim_{n\to \infty}{X_n(\omega)}= 0$ for all $\omega$.
\end{Example}

To prove the MCT, we first show the following.
\begin{Lemma} \label{wk4:lem:MCT_pre}
Suppose that $X\geq 0$ is a simple random variable and $B_n \subset B_{n+1}$ for $n \geq 1$. Let $B=\bigcup_{n\geq 1} {B_n}$. We have
\begin{align*}
\lim_{n \to \infty}\E[X \indicatore{B_n}]=\E[X \indicatore{B}].
\end{align*}
\end{Lemma}
\begin{proof}
Suppose
\begin{align*}
X &= \sum_{i=1}^m {x_i \indicatore{A_i}},
\intertext{then}
X\indicatore{B_n} &=\sum_{i=1}^m {x_i \indicatore{A_i\cap B_n}},
\end{align*}
and from \cref{wk4:lem:B-exp}, we obtain
\begin{align*}
\E[X \indicatore{B_n}]&= \sum_{i=1}^m {x_i \P({A_i\cap B_n})}.
\end{align*}
Since $\E[X \indicatore{B_n}] \leq \E[X \indicator {B_{n+1}}]$ is an increasing sequence in $\Real$, its limit exists (can be $\infty$). Taking limit as $n\to\infty$ on both sides, we obtain
\begin{align*}
\lim_{n \to \infty}\E[X \indicatore{B_n}]
&= \sum_{i=1}^m x_i \lim_{n\to\infty} \P(A_i\cap B_n)\\
&= \sum_{i=1}^m x_i \P(\bigcup_{n\geq 1}\braces*{A_i\cap B_n})\\
&=\sum_{i=1}^m x_i \P(A_i\cap B)\\
&=\E[X \indicatore{B}].
\end{align*}
\end{proof}

\begin{proof}[Proof of MCT]
Since $X_n\leq X$ for all $n\geq1$, we have $\E X_n \leq \E X$ from \cref{wk4:lem:XlessY}. As $\E X_n \leq \E X_{n+1}$ is an increasing bounded sequence, its limit exists and
\begin{align*}
\lim_{n\to\infty} \E X_n \leq \E X.
\end{align*}

Let $Z$ be a simple random variable such that $0\leq Z\leq X$. Let $0<\rho<1$, and define
\begin{align*}
B_n&=\set*{\omega \given X_n(\omega)\geq \rho Z(\omega)}.
\end{align*}
We have $B_n \subset B_{n+1}$. Furthermore, for $n$ sufficiently large, we have $X_{n}(\omega)\geq \rho X(\omega) \geq \rho Z(\omega)$ $\forall \omega\in\Omega$ (i.e., $B_n=\Omega$), so that $\bigcup_{j\geq1} B_j = \Omega$.  For such $n$, we then have
\begin{align*}
&\rho Z \indicatore{B_{n}} \leq X_n \indicatore{B_{n}} \leq X_m,\ \forall m\geq n,\\
& \rho \E[Z\indicatore{B_{n}}] \leq \E X_m,
\end{align*}
from \cref{wk4:def:expectation_nz} since $Z\indicatore{B_{n}}$ is simple. Letting $m\to \infty$,
\begin{align*}
&\rho \E [Z \indicatore{B_{n}}] \leq \lim_{m \to \infty}\E X_m,
\end{align*}
and $n \to \infty$, we have from \cref{wk4:lem:MCT_pre},
\begin{align*}
\rho \E [Z\indicatore{\bigcup_{n\geq1} B_n}]&\leq \lim_{m \to \infty}\E X_m,\\
\rho \E [Z] &\leq \lim_{m \to \infty}\E X_m.
\end{align*}
Taking sup over all simple $Z \leq X$, we have
\begin{align*}
& \rho \E X \leq \lim_{n \to \infty}\E X_n,
\end{align*}
and $\rho \to 1$ completes the proof.
\end{proof}

The following procedure gives an explicit construction of a sequence of increasing simple random variables that approximate $X \geq 0$. For each $k\geq 1$ and $0 \leq j <2^{2k}$, let
\begin{align*}
B_{k,j} &=\set*{\omega \given \frac{j}{2^k}<X(\omega)\leq \frac{j+1}{2^k}},\\
B_{k,2^{2k}} &=\set*{\omega \given X(\omega)>2^k},
\end{align*}
and
\begin{align*}
Z_k &=\sum_{j=0}^{2^{2k}}{\frac{j}{2^k}\indicatore{B_{k,j}}}.
\end{align*}
We have $Z_k(\omega) \leq Z_{k+1}(\omega)$ and 
\begin{align*}
& 0 \leq X(\omega)-Z_k(\omega)\leq 2^{-k},\ \forall \omega\ \ST X(\omega)\leq 2^{k}, \\
& Z_k(\omega)=2^k, \ X(\omega) > 2^k.
\end{align*}
Therefore $Z_k(\omega)\uparrow X(\omega)$ as $k\to\infty$. We finally have all the tools to prove \cref{wk4:additivity_exp}.

\begin{proof}[Proof of \cref{wk4:additivity_exp}]
For any $X,Y\geq 0$, $\exists$ simple $X_n \uparrow X$ and simple $Y_n \uparrow Y$. We then have $X_n+Y_n \uparrow X+Y$ and $\E[X_n+Y_n] = \E X_n+ \E Y_n$. From the MCT, taking $n\to\infty$, we obtain $\E[X+Y] = \E X+ \E Y$. 
\end{proof}

Note that additivity of expectations for general random variables follows from \cref{wk4:additivity_exp} since $\E X = \E X^+ - \E X^-$.


The steps in the proof of \cref{wk4:additivity_exp} are quite standard in measure theory:
\begin{enumerate}[(i)]
\item Prove for simple random variables.
\item Extend to non-negative random variables $X$ by using simple random variables $\uparrow$ $X$, then apply MCT.
\item Extend to general $X=X^+-X^-$.
\end{enumerate}

\section{Fatou's Lemma and the DCT}

\begin{Lemma}[Fatou's Lemma] \label{Lemma:Fatou}
Suppose that $X_n\geq 0$ for $n\geq 1$. We have
\begin{align*}
\E[\liminf_{n \to \infty}{X_n}] \leq  \liminf_{n \to \infty}\E X_n.
\end{align*}
\end{Lemma}
\begin{proof}
Let $Y_k= \inf_{n\geq k} X_n\geq 0$. We have $Y_k \leq Y_{k+1}$ and $Y_k \leq X_n$, $\forall n\geq k$ so that 
\begin{align*}
\E Y_k &\leq \inf_{n\geq k} {\E X_n} \\
\lim_{k\to \infty}{\E Y_k} &\leq \lim_{k\to \infty}{\inf_{n\geq k} {\E X_n}}= \liminf_{n\to \infty}{\E X_n}.
\end{align*}
From MCT and \cref{wk1:limsup_infsup}, we have
\begin{align*}
\lim_{k\to \infty}{\E Y_k}
&=\E[\lim_{k\to \infty} {Y_k}]\\
&=\E[\lim_{k\to \infty} {\inf_{n\geq k}{X_n}}]\\
&=\E[\liminf_{n\to \infty}{ X_n}],
\end{align*}
and we obtain
\begin{align*}
\E[\liminf_{n \to \infty}{X_n}] &\leq  \liminf_{n \to \infty}\E X_n.
\end{align*}
\end{proof}
A similar proof as that for \cref{Lemma:Fatou} shows that if $X_n \leq Y$ where $\E|Y|<\infty$, then
\begin{align*}
\E[\limsup_{n\to\infty} X_n] \geq \limsup_{n\to\infty} \E X_n.
\end{align*}

\begin{Theorem}[Dominated Convergence Theorem (DCT)]\label{Dominated Convergence Theorem}
Suppose $|X_n(\omega)| \leq Y(\omega)$ $\forall \omega\in\Omega$, and $\E |Y|< \infty$. If $\lim_{n\to \infty}{X_n}=X$, then $\E X$ exists, $\E|X|\leq \E Y$ and 
\begin{align*}
\lim_{n \to \infty}{\E X_n} = \E X. 
\end{align*}
\end{Theorem}
\begin{proof}
From \cref{wk4:lem:XlessY}, we have $\E|X_n|\leq \E Y$, and Fatou's Lemma (\cref{Lemma:Fatou}) yields $\E|X|\leq \E Y$. Therefore, $\E X$ exists. Since $Y + X_n \geq 0$, from Fatou's Lemma, we have
\begin{align}
& \E [\liminf_{n \to \infty}{(Y+X_n)}]\leq  \liminf_{n \to \infty}{ \E [Y+X_n]}.\label{YplusXn}
\end{align}
We also have
\begin{align*}
& \E [\liminf_{n \to \infty}{(Y+X_n)}]= \E [Y+X]=\E Y +\E X,
\end{align*}
and 
\begin{align*}
& \liminf_{n \to \infty}{ \E [Y+X_n]}=\E Y +\liminf_{n \to \infty}{ \E [X_n]},
\end{align*}
so that \cref{YplusXn} yields
\begin{align*}
& \E X \leq \liminf_{n \to \infty}{ \E X_n}.
\end{align*}
Similarly, we have $Y-X_n\geq 0$ and Fatou's Lemma implies that
\begin{align*}
& \E [\liminf_{n \to \infty}{(Y-X_n)}]\leq  \liminf_{n \to \infty}{ \E [Y-X_n]},\\
& \E X \geq \limsup_{n \to \infty}{ \E X_n}.
\end{align*}
Therefore, $\E X = \lim_{n \to \infty}{ \E X_n}.$
\end{proof}

\begin{Lemma}[Scheffe] \label{Lemma:Scheffe's Lemma}
Suppose that $X_n\geq 0$ for $n\geq1$, $\lim_{n\to \infty}{X_n}= X$, both $X_n$ and $X$ are integrable, and $\lim_{n\to \infty}{\E X_n}= \E X$. Then, $\lim_{n\to \infty}{\E |X_n-X|}=0$.
\end{Lemma}
\begin{proof}
Since $X_n\geq 0$ and $X\geq0$, we have
\begin{align*}
&0\leq (X_n-X)^- \leq X.
\end{align*}
As $\E X <\infty$ and $(X_n-X)^- \to 0$ since $X_n \to X$, DCT yields $\E(X_n-X)^- \to 0$. From
\begin{align*}
&\E (X_n-X)=\E (X_n-X)^+-\E (X_n-X)^-,
\end{align*}
and $\E (X_n-X) \to 0$, we obtain $\E (X_n-X)^+ \to 0$ and
\begin{align*}
\E |X_n-X|&=\E (X_n-X)^++\E (X_n-X)^-\to 0.
\end{align*}
\end{proof}
Hence, any sequence of r.v.s satisfying the conditions of the DCT also converges in $L^1$. 

\section{Notes}

Suppose $f : (\Omega, \calA, \mu) \mapsto (\Real, \calB(\Real))$ is a non-negative measurable function, where $\mu$ is a general measure (i.e., does not have the restriction that $\mu(\Omega)=1$), then the Lebesgue integral of $f$ over a measurable set $A \in \calA$ denoted by
\begin{align*}
\int_A f \ud\mu
\end{align*}
is defined in exactly the same way as in \cref{wk4:def:expectation_nz}. In fact, the proofs of MCT, Fatou's Lemma and DCT do not require that the underlying measure is a probability measure! Therefore, all the results we have seen so far are true for the general Lebesgue integral. If $f$ is a.e. continuous, then its Lebesgue integral and Riemann integral give the same value, but their constructions are different. 

For $p\geq 1$, we define
\begin{align*}
L^p(\Omega, \calA, \mu) = \set*[\vert]{ f : (\Omega, \calA, \mu) \mapsto (\Real, \calB(\Real)) \given \int_\Omega |f|^p \ud\mu < \infty}.
\end{align*}
If obvious from the context, we usually shorten the notation to $L^p(\mu)$ or $L^p(\Omega)$. An integrable function means it is in $L^1(\mu)$. Using Minkowski’s inequality, it can be shown that 
\begin{align*}
\norm{f}_p = \parens*{\int |f|^p \ud\mu}^{1/p}
\end{align*}
is a norm, hence $L^p(\mu)$ is a normed space.

If $g$ is a measurable function, then from  \cref{wk4:def:expectation}, we have
\begin{align*}
\E g(X) & =\int g(X(\omega)) \ud\P(\omega).
\end{align*}
Letting $x=X(\omega)$, we obtain
\begin{align}
\E g(X) & = \int g(x) \ud\P \circ X^{-1}(x)\nn
& = \int g(x) \ud\P_{X}(x). \label{wk4:Eg}
\end{align}
Note that $X^{-1}(B)=\{\omega :X(\omega) \in B\}$ is defined as the inverse map for Borel sets, and is different from a traditional inverse function, which requires bijectiveness. In particular, $X^{-1}(\cdot)$ is always well-defined.

For discrete random variables $X \in \{x_1,x_2,\ldots\}$, suppose $\P_X(\{x_i\})=p_i$, a discrete measure. Then the expectation \cref{wk4:Eg} reduces to a sum $\sum_{i}{g(x_i) p_i}$. For ``continuous'' random variables you are familiar with in undergrad probability courses, we need a definition.

\begin{Definition}\label{wk4:def:expectation continuous}
If $\mu (A) =0 \implies \nu(A)=0$ for all measurable sets $A$, then we say that $\nu$ is absolutely continuous w.r.t. $\mu$ and write $\nu \ll \mu$.
\end{Definition}

A measure $\mu$ is $\sigma$-finite if $\Omega= \bigcup_{i\geq 1}{\Omega_i}$, where the $\Omega_i$ are disjoint and $\mu(\Omega_i)<\infty$.

\begin{Theorem}[Radon-Nikodym]\label{wk4:Radon-Nikodym Theorem}
Suppose the $\nu$ is finite and $\mu$ is $\sigma$-finite. If $\nu \ll \mu$, then there exists measurable $f \geq 0$ with $ \int {|f|} \ud \mu <\infty$ (i.e., $f\in L^1(\mu)$) such that
\begin{align*}
\nu(A) & = \int_{A} f \ud\mu.
\end{align*}
\end{Theorem}
Notation-wise, we usually write $f = \ddfrac{\nu}{\mu}$, which is called the Radon-Nikodym derivative. 

\begin{Definition}
$X$ is a continuous random variable if $\exists f \in L^1(\Real,\lambda)$, where $\lambda$ is the Lebesgue measure, i.e., $\lambda([a,b])=b-a$, s.t.\
\begin{align*}
\P_X(A)& =\int_{A} f \ud\lambda =\int_{A} f(x) \ud x
\end{align*}
for all measurable $A$. The function $f$ is called the probability density function (pdf) of $X$. 
\end{Definition}
We also have $ \E g(X) = \int g(x) \ud \P_X(x) = \int g(x) f(x) \ud x$.



%\bibliography{mybib}
\bibliographystyle{alpha}

\end{document}
